{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName = \"spark_ml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: Kmeans (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "data = sc.textFile(\"./datasets/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(\" \")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model (cluster the data)\n",
    "clusters = KMEans.train(parsedData, 2, maxIterations=10, runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in point-center]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x,y: x+y)\n",
    "\n",
    "print(\"Within Set Sum of Squared Errors = \", WSSSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "clusters.save(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: DecisionTree (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data file into an RDD of LabeledPoint\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# Empty categoricalFeaturesInfo indicates all features are continuous\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={}, impurity='gini', maxDepth=5, maxBins=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda Ip: Ip.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda v,p: v!=p).count()/float(testData.count())\n",
    "\n",
    "print('Test Error =', testErr)\n",
    "print('Learned classification tree model:', model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "\n",
    "sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento iterativo con Spark: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014-03-15:10:10:20|Sorrento F41L|8cc3b47e-bd01-4482-b500-28f2342679af|7|24|39|enabled|disabled|connected|55|67|12|33.6894754264|-117.543308253']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"/tmp/curso/devicestatus.txt\")\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33.6894754264, -117.543308253)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = data.map(lambda x: (float(x.split(\"|\")[12]), float(x.split(\"|\")[13]))).filter(lambda x: x != (0,0))\n",
    "points.cache() # save in cache as will use many times\n",
    "points.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a point p and an array of points, return the index in the array of the point closest to p\n",
    "def closestPoint(p, points):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    # for each point in the array, calculate the distance to the test point, then return\n",
    "    # the index of the array point with the smallest distance\n",
    "    for i in range(len(points)):\n",
    "        dist = distanceSquared(p,points[i])\n",
    "        if dist < closest:\n",
    "            closest = dist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "# The squared distances between two points\n",
    "def distanceSquared(p1,p2):  \n",
    "    return (p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2\n",
    "\n",
    "# The sum of two points\n",
    "def addPoints(p1,p2):\n",
    "    return [p1[0] + p2[0], p1[1] + p2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # number of means (center points of clusters ) to find\n",
    "convergeDist = 0.1 # the threshold \"distance\" between iterations at which we decide we are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(34.0830381107, -117.960562808),\n",
       " (34.2480006224, -117.931551969),\n",
       " (34.3719071909, -117.850561452),\n",
       " (38.4399201611, -121.019109788),\n",
       " (33.6093366014, -111.769407277)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting points\n",
    "# takeSample(withReplacement, num, seed = None)\n",
    "kPoints = loc.takeSample(False, K, 42)\n",
    "kPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between iterations: 4.168687670254237\n",
      "Distance between iterations: 3.745890322578019\n",
      "Distance between iterations: 1.666217245297977\n",
      "Distance between iterations: 0.5097386038620183\n",
      "Distance between iterations: 0.8317221662820514\n",
      "Distance between iterations: 1.5694681458063235\n",
      "Distance between iterations: 2.3535047857216482\n",
      "Distance between iterations: 0.9390267476897196\n",
      "Distance between iterations: 0.06568718669542292\n",
      "Final center points:  [[34.272325629615345, -117.8304037284927], [38.056132558472484, -121.205019936655], [36.70143101357855, -114.65485840414931], [43.924776157124676, -121.37436192818824], [33.687196560692406, -111.04680557861003]]\n"
     ]
    }
   ],
   "source": [
    "dist = float(\"+inf\")\n",
    "while dist > convergeDist:\n",
    "    # for each point, find the index of the closest kpoint.  map to (index, (point,1))\n",
    "    closest = points.map(lambda point : (closestPoint(point, kPoints), (point, 1)))\n",
    "    # for each key (k-point index), reduce by adding the coordinates and number of points\n",
    "    point_stats = closest.reduceByKey(lambda accum, n: (addPoints(accum[0],n[0]),accum[1]+n[1]))\n",
    "    # for each key (k-point index), find a new point by calculating the average of each closest point\n",
    "    newPoints = point_stats.map(lambda x: (x[0],[x[1][0][0]/x[1][1],x[1][0][1]/x[1][1]])).collect()\n",
    "    # calculate the total of the distance between the current points and new points\n",
    "    dist=0\n",
    "    for i, point in newPoints: \n",
    "        dist += distanceSquared(kPoints[i],point)\n",
    "    print(\"Distance between iterations:\", dist)\n",
    "    # Copy the new points to the kPoints array for the next iteration\n",
    "    for i, point in newPoints: \n",
    "        kPoints[i] = point\n",
    "        \n",
    "print(\"Final center points: \",kPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLlib RDD-based API (to be deprecated in Spark 3.0) vs Spark MLlib DataFrame-based API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample pipeline and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib RDD-based API (Spark MLlib)\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "obs = sc.parallelize(\n",
    "    [LabeledPoint(1.0, [1.0, 0.0, 10.0])]), ...,\n",
    "     LabeledPoint(1.0, [1.0, 0.0, -0.5])])\n",
    "training, test = obs.randomSplit([0.6,0.4]) # random\n",
    "# regression\n",
    "model = LogisticRegressionWithSGD.train(training)\n",
    "valuesAndPreds = test.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "print(\"MAE = \", metrics.meanAbsoluteError)\n",
    "print(\"MSE = \", metrics.meanSquaredError)\n",
    "print(\"RMSE = \", metrics.rootMeanSquaredError)\n",
    "print(\"R-squared = \", metrics.r2)\n",
    "print(\"Explained variance = \", metrics.explainedVariance)\n",
    "# classification\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    " # binary classification metrics\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"AUC CP = \", metrics.areaUnderPR)\n",
    "print(\"AUC ROC = \", metrics.areaUnderROC)\n",
    " # accuracy\n",
    "tests = predictionAndLabels.map(lambda pair:pair[0]==pair[1]).collect()\n",
    "nTests = len(tests)\n",
    "nHits = tests.count(True)\n",
    "accuracy = float(nHits)/nTests\n",
    "print(\"Accuracy = \", accuracy)\n",
    " # multiclass metrics\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"Precision = \", metrics.precision(1.0))\n",
    "print(\"Recall = \", metrics.recall(1.0))\n",
    "print(\"F1 = \", metrics.fMeasure(1.0, beta=1.0))\n",
    "newobs = [1.0, 0.0, 10.0]\n",
    "model.predict(newobs)\n",
    "\n",
    "# MLlib DataFrame-based API (Spark Ml)\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "obsdf = sqlContext.createDataFrame([\n",
    "    (1.0, Vectors.dense([1.0, 0.0, 10.0])), ...\n",
    "    (1.0, Vectors.dense([1.0, 0.0, -0.5]))\n",
    "    ], [\"label\", \"features\"])\n",
    "training, test = obsdf.randomSplit([0.6, 0.4])\n",
    "# regression\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(training)\n",
    "predictedobsdf = lrModel.transform(test)\n",
    "predictionAndLabels = predictedobsdf.select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "MAE = evaluator.evaluate(predictionAndLabels, {evaluator.metricname: \"mae\"})\n",
    "MSE = evaluator.evaluate(predictionAndLabels, {evaluator.metricname: \"mse\"})\n",
    "RMSE = evaluator.evaluate(predictionAndLabels, {evaluator.metricname: \"rmse\"})\n",
    "# classification\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(training)\n",
    "predictedobsdf = lrModel.transform(test)\n",
    "predictionAndLabels = predictedobsdf.select(\"probability\", \"label\")\n",
    " # binary classification metrics\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\")\n",
    "AUPR = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"areaUnderPR\"})\n",
    "AUROC = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"areaUnderROC\"})\n",
    " # accuracy\n",
    "predictionAndLabels = predictedobsdf.select(\"prediction\", \"label\")\n",
    "tests = predictionAndLabels.rdd.map(lambda pair: pair[0]==pair[1]).collect()\n",
    "nTests = len(tests)\n",
    "nHits = tests.count(True)\n",
    "accuracy = float(nHits)/nTests\n",
    "# multiclass\n",
    "predictionAndLabels = predictedobsdf.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "precision = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"precision\"})\n",
    "recall = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"recall\"})\n",
    "F = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"f1\"})\n",
    "newobsdf = sqlContext.createDataFrame([\n",
    "    (Vectors.dense([1.0, 0.0, 10.0]))\n",
    "    ], [\"features\"])\n",
    "lrModel.transform(newobsdf).select(\"prediction\").show() # predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algoritms regression - Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib RDD-based API (Spark MLlib): https://spark.apache.org/docs/latest/mllib-linear-methods.html\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.mllib import RegressionMetrics\n",
    "# load file\n",
    "csvfile = \"file:///.../student-mat.csv\"\n",
    "data = sc.textFile(csvfile)\n",
    "# numeric transformation\n",
    "data = data.map(lambda x: x.replace(\"M\", \"1\"))\n",
    "data = data.map(lambda x: x.replace(\"F\", \"2\"))\n",
    "data = data.map(lambda x: x.replace(\"yes\", \"1\"))\n",
    "data = data.map(lambda x: x.replace(\"no\", \"0\"))\n",
    "datacsv = data.map(lambda x: x.split(\";\"))\n",
    "    # non binary version\n",
    "# LabeledPoint generation\n",
    "labeleddata = datacsv.map(lambda x: LabeledPoint(x[8], x[:7]))\n",
    "# train-test sets generation\n",
    "training, test = labeleddata.randomSplit([0.8, 0.2])\n",
    "    # binary version\n",
    "def binarize(v):\n",
    "    if int(v[8])>10:\n",
    "        v[8] = 0\n",
    "    else:\n",
    "        v[8] = 1\n",
    "    return v\n",
    "datacsvbin = datacsv.map(binarize)\n",
    "# LabeledPoint generation\n",
    "labeleddatabin = datacsvbin.map(lambda x: LabeledPoint(x[8], x[:7]))\n",
    "# train-test sets generation\n",
    "trainingbin, testbin = labeleddatabin.randomSplit([0.8, 0.2])\n",
    "# training\n",
    "model = LinearRegressionWithSGD.train(training) # with Stochastic Gradient Descent\n",
    "print(model.weights)\n",
    "# evaluation\n",
    "valuesAndPreds = test.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "print(\"MAE = \", metrics.meanAbsoluteError)\n",
    "\n",
    "\n",
    "# MLlib DataFrame-based API (Spark Ml): \n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# load file\n",
    "csvfile = \"file:///.../student-mat.csv\"\n",
    "sqlContext = SQLContext(sc)\n",
    "    # non binary version\n",
    "# dataframe creation\n",
    "dfs = datacsv.map(lambda x: (float(x[8]), Vectors.dense(x[0:7])))\n",
    "datadf = sqlContext.createDataFrame(dfs, [\"label\", \"features\"])\n",
    "# train-test sets generation\n",
    "traindf, testdf = datadf.randomSplit([0.8, 0.2])\n",
    "    # binary version\n",
    "def binarize(v):\n",
    "    if int(v[8])>10:\n",
    "        v[8] = 0\n",
    "    else:\n",
    "        v[8] = 1\n",
    "    return v\n",
    "datacsvbin = datacsv.map(binarize)\n",
    "# dataframe creation\n",
    "dfsbin = datacsvbin.map(lambda x: (float(x[8]), Vectors.dense(x[0:7])))\n",
    "datadfbin = sqlContext.createDataFrame(dfsbin, [\"label\", \"features\"])\n",
    "# train-test sets generation\n",
    "traindfbin, testdfbin = datadfbin.randomSplit([0.8, 0.2])\n",
    "# training\n",
    "trainer = LinearRegression(traindfbin) # it just supports binary classification\n",
    "model = trainer.fit(traindfbin)\n",
    "print(\"Coefficients: \", str(model.coefficients))\n",
    "# evaluation\n",
    "predicteddf = model.transform(testdfbin)\n",
    "predictionAndLabels = predicteddf.select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "MAE = evaluator.evaluate(predictionAndLabels), {evaluator.metricName:\"mae\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algoritms regression - Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib RDD-based API (Spark MLlib): https://spark.apache.org/docs/latest/mllib-linear-methods.html\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "# model\n",
    "model = DecisionTree.trainRegressor(training, categoricalFeaturesInfo={}, impurity='variance')\n",
    "print(model.toDebugString()) # shows DecisionTreeModel info\n",
    "# evaluation\n",
    "predictions = model.predict(test.map(lambda x: x.features)) # need to do it separately, does not support a map with a prediction inside\n",
    "valuesAndPreds = test.map(lambda lp: lp.label).zip(predictions)\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "print(\"MAE = \", metrics.meanAbsoluteError)\n",
    "\n",
    "# MLlib DataFrame-based API (Spark Ml): \n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# model \n",
    "# this API requires at least that Y is indexed\n",
    "labelIndexer = StringIndexer(inputCol = \"label\", outputCol = \"indexedLabel\").fit(datadf)\n",
    "featureIndexer = VectorIndexer(inputCol = \"features\", outputCol = \"indexedFeatures\", maxCategories=21).fit(datadf)\n",
    "trainer = StringIndexer(labelCol = \"indexedLabel\", featurestCol = \"indexedFeatures\")\n",
    "pipeline = Pipeline(stages = [labelIndexer, featureIndexer, trainer])\n",
    "model = pipeline.fit(trainingdf)\n",
    "# evaluation\n",
    "predicteddf = model.transform(testdf)\n",
    "predictionAndLabels = predicteddf.select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "MAE = evaluator.evaluate(predictionAndLAbels, {evaluator.metricName : \"mae\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algoritms regression - Decision Tree Ensembles (Random Forest & Gradient Boosting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib RDD-based API (Spark MLlib): https://spark.apache.org/docs/latest/mllib-linear-methods.html\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "# model\n",
    "modelRF = RandomForest.trainRegressor(training, categoricalFeaturesInfo={}, numTrees=3)\n",
    "modelFB = GradientBoostedTrees.trainRegressor(training, categoricalFeaturesInfo={}, numIterations=3)\n",
    "print(model.toDebugString()) # shows DecisionTreeModel info\n",
    "# evaluation\n",
    "predictions = model.predict(test.map(lambda x: x.features)) # need to do it separately, does not support a map with a prediction inside\n",
    "valuesAndPreds = test.map(lambda lp: lp.label).zip(predictions)\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "print(\"MAE = \", metrics.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algoritms classification - Support Vector Machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib RDD-based API (Spark MLlib): https://spark.apache.org/docs/latest/mllib-linear-methods.html\n",
    "from pyspark.mllib.classification import SVMwithSGD, SVMModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# model\n",
    "# SVMwithSGD just supports binary problems\n",
    "model = SVMWithSGD.train(trainingbin, iterations=100)\n",
    "print(model.weights)\n",
    "# evaluation\n",
    "predictionAndLabels = testbin.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"F1 = \", metrics.fMeasure())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algoritms classification - Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib RDD-based API (Spark MLlib): https://spark.apache.org/docs/latest/mllib-linear-methods.html\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# model\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "print(model.pi) # model class probs\n",
    "print(model.theta) # model feature probs\n",
    "# evaluation\n",
    "predictionAndLabels = testbin.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"F1 = \", metrics.fMeasure())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALS algorithm (Alternating Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating, ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific structure for recommendation Rating(user, product, rating)\n",
    "ratings = sc.parallelize(\n",
    "            [Rating(1,1,1.0),\n",
    "             Rating(1,2,1.0),\n",
    "             Rating(2,1,1.0),\n",
    "             Rating(2,3,1.0),\n",
    "             Rating(3,2,1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model = ALS.train(ratings, rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=0.9955445398917746),\n",
       " Rating(user=1, product=2, rating=0.9843777028424281),\n",
       " Rating(user=1, product=3, rating=-0.3855545946576022)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction\n",
    "model.recommendProducts(1,3) # 3 products for user 1\n",
    "# careful because it will give us back also the products already consumed --> need to filter them in app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(sc,”rutaDeMiArchivoDeModelo”)\n",
    "model = LinearRegressionWithSGD.load(sc ,”rutaDeMiArchivoDeModelo”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "#### Spark MLlib DataFrame-based API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row, DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(age=26, height=1.87, prediction=1.469387755102058)\n",
      "Row(age=28, height=1.99, prediction=2.591836734693874)\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "training = sqlContext.createDataFrame([\n",
    "            (29, 1.85, 1.0),\n",
    "            (26, 1.96, 3.0),\n",
    "            (24, 2.05, 4.0),\n",
    "            (30, 1.98, 3.0),\n",
    "            (29, 1.91, 2.0),\n",
    "            (19, 2.17, 5.0)],\n",
    "            [\"age\", \"height\", \"label\"])\n",
    "\n",
    "# Configurar un pipeline con tres etapas: bucketizer, assembler, lr\n",
    "bucketizer = Bucketizer(splits=[-float(\"inf\"), 1.90, 2.00, float(\"inf\")], inputCol=\"height\", outputCol=\"bucketHeight\")\n",
    "assembler = VectorAssembler(inputCols=[\"age\", \"bucketHeight\"],outputCol=\"features\")\n",
    "lr = LinearRegression()\n",
    "pipeline = Pipeline(stages=[bucketizer, assembler, lr])\n",
    "\n",
    "# Entrenamiento\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Carga de datos de predicción = igual encabezado sin “label”\n",
    "test = sqlContext.createDataFrame([(26, 1.87),(28, 1.99)],[\"age\", \"height\"])\n",
    "\n",
    "# Predicción\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"age\", \"height\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text=\"a B c 1 he's\", words=['a', 'b', 'c', '1', \"he's\"])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un DataFrame con la columna “text”\n",
    "df = sqlContext.createDataFrame([(\"a B c 1 he's\",)], [\"text\"])\n",
    "# Creamos el tokenizador, con columna origen y columna destino\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# No hay método fit(), solo transform()\n",
    "tokenizer.transform(df).head() # Se observa que pasa a minúsculas y no reconoce “’” como separador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text='Tengo 2, y tú no', words=['tengo', '2,', 'y', 'tú', 'no'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el tokenizador\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# Creamos un DataFrame\n",
    "df = sqlContext.createDataFrame([(\"Tengo 2, y tú no\",)], [\"text\"])\n",
    "reTokenizer.transform(df).head() # No ha reconocido el acento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(text='correo correo@ejemplo.com', words=['correo', 'correo@ejemplo.com'])\n",
      "Row(text='correo correo@ejemplo.com', words=['correo', 'correo', 'ejemplo', 'com'])\n",
      "Row(text='correo correo@ejemplo.com', words=[' ', '@', '.'])\n"
     ]
    }
   ],
   "source": [
    "# Creamos un nuevo DataFrame\n",
    "df = sqlContext.createDataFrame([('correo correo@ejemplo.com',)], [\"text\"])\n",
    "\n",
    "# Expresión regular por defecto\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "print(reTokenizer.transform(df).head())\n",
    "\n",
    "# Expresión regular = blancos, “@” y “.”\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"[\\s@\\.]+\", gaps=True)\n",
    "print(reTokenizer.transform(df).head())\n",
    "\n",
    "# Misma expresión regular, pero devuelve los separadores\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"[\\s@\\.]+\", gaps=False)\n",
    "print(reTokenizer.transform(df).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b', 'b c', 'c d', 'd e'])\n",
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b c d', 'b c d e'])\n"
     ]
    }
   ],
   "source": [
    "# Creamos un DataFrame\n",
    "df = sqlContext.createDataFrame([Row(inputTokens=[\"a\", \"b\", \"c\", \"d\", \"e\"])])\n",
    "\n",
    "# Creamos un objeto NGram con longitud de secuencia 2\n",
    "ngram = NGram(n=2, inputCol=\"inputTokens\", outputCol=\"nGrams\")\n",
    "print(ngram.transform(df).head())\n",
    "\n",
    "# Cambiamos la longitud de secuencia a 4\n",
    "print(ngram.setParams(n=4).transform(df).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], outputTokens=['c', 'd', 'e'])\n",
      "caseSensitive: whether to do a case sensitive comparison over the stop words (default: False)\n",
      "inputCol: input column name. (current: inputTokens)\n",
      "inputCols: input column names. (undefined)\n",
      "locale: locale of the input. ignored when case sensitive is true (default: en_GB)\n",
      "outputCol: output column name. (default: StopWordsRemover_727d836dd65e__output, current: outputTokens)\n",
      "outputCols: output column names. (undefined)\n",
      "stopWords: The words to be filtered out (default: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would'], current: ['a', 'b'])\n"
     ]
    }
   ],
   "source": [
    "# Creamos un DataFrame\n",
    "df = sqlContext.createDataFrame([Row(inputTokens=[\"a\", \"b\", \"c\", \"d\", \"e\"])])\n",
    "# Creamos un objeto StopWordsRemover\n",
    "stopwords = StopWordsRemover(inputCol=\"inputTokens\", outputCol=\"outputTokens\", stopWords=[\"a\",\"b\"])\n",
    "print(stopwords.transform(df).head())\n",
    "\n",
    "# Estos objetos incluyen una lista de parada por defecto para el inglés\n",
    "print(stopwords.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|               words|\n",
      "+--------------------+--------------------+\n",
      "|You are a friend ...|[you, are, a, fri...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "Row(text='You are a friend of mine', words=['you', 'are', 'a', 'friend', 'of', 'mine'], filteredWords=['friend', 'mine'])\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame([(\"You are a friend of mine\",)], [\"text\"])\n",
    "\n",
    "# Tokenización del texto de entrada\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df2=tokenizer.transform(df)\n",
    "df2.show()\n",
    "\n",
    "# Eliminación de palabras en la lista de parada\n",
    "stopwords = StopWordsRemover(inputCol=\"words\", outputCol=\"filteredWords\")\n",
    "print(stopwords.transform(df2).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexación y sistemas de pesos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------------+\n",
      "|label|raw            |vectors                  |\n",
      "+-----+---------------+-------------------------+\n",
      "|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+-----+---------------+-------------------------+\n",
      "\n",
      "binary: Binary toggle to control the output vector values. If True, all nonzero counts (after minTF filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False (default: False)\n",
      "inputCol: input column name. (current: raw)\n",
      "maxDF: Specifies the maximum number of different documents a term could appear in to be included in the vocabulary. A term that appears more than the threshold will be ignored. If this is an integer >= 1, this specifies the maximum number of documents the term could appear in; if this is a double in [0,1), then this specifies the maximum fraction of documents the term could appear in. Default (2^63) - 1 (default: 9.223372036854776e+18)\n",
      "minDF: Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer >= 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default 1.0 (default: 1.0)\n",
      "minTF: Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer >= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document's token count). Note that the parameter is only used in transform of CountVectorizerModel and does not affect fitting. Default 1.0 (default: 1.0)\n",
      "outputCol: output column name. (default: CountVectorizer_7cfd0bba7583__output, current: vectors)\n",
      "vocabSize: max size of the vocabulary. Default 1 << 18. (default: 262144)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Creamos un DataFrame de textos como secuencias de palabras, con una etiqueta\n",
    "df = sqlContext.createDataFrame([(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],[\"label\", \"raw\"])\n",
    "\n",
    "# Definimos el objeto CountVectorizer con sus parámetros\n",
    "cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "\n",
    "# Entrenamos = generamos el vocabulario\n",
    "model = cv.fit(df)\n",
    "\n",
    "# Transformamos = representamos respecto a dicho vocabulario\n",
    "model.transform(df).show(truncate=False)\n",
    "\n",
    "# Imprimimos el Vocabulario\n",
    "sorted(map(str, model.vocabulary))\n",
    "\n",
    "print(cv.explainParams())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10, {5: 1.0, 7: 1.0, 8: 2.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "# Creamos un DataFrame de listas de palabras\n",
    "df = sqlContext.createDataFrame([([\"a\", \"b\", \"c\", \"c\"],)], [\"words\"])\n",
    "# Creamos un objeto HashingTF con 10 atributos\n",
    "hashingTF = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"features\")\n",
    "hashingTF.transform(df).head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computa pesos IDF sobre una colección de vectores de documentos\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import IDF, IDFModel\n",
    "df = sqlContext.createDataFrame([(DenseVector([1.0, 2.0]),),(DenseVector([0.0, 1.0]),), (DenseVector([3.0, 0.2]),)], [\"tf\"])\n",
    "idf = IDF(minDocFreq=3, inputCol=\"tf\", outputCol=\"idf\")\n",
    "model = idf.fit(df)\n",
    "model.transform(df).head().idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classifier - Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext, Row, DataFrame\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Creamos una colección de documentos que son ternas (id, text, label)\n",
    "training = sqlContext.createDataFrame([\n",
    "        (0, \"me gusta el iphone\", 1.0),\n",
    "        (1, \"odio el samsung galaxy\", 0.0),\n",
    "        (2, \"me gusta el galaxy\", 1.0),\n",
    "        (3, \"no me gusta el nokia communicator\", 0.0)], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configuramos un pipeline de tres etapas: tokenizer, hashingTF, y lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Entrenamos el pipeline sobre los datos de entrenamiento\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='me gusta spark', prediction=1.0)\n",
      "Row(id=5, text='odio spark', prediction=0.0)\n",
      "Row(id=6, text='no me gusta la fruta', prediction=1.0)\n",
      "Row(id=7, text='odio la fruta', prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Creamos una colección de prueba, que son pares (id, text)\n",
    "test = sqlContext.createDataFrame([\n",
    "    (4, \"me gusta spark\"),\n",
    "    (5, \"odio spark\"),\n",
    "    (6, \"no me gusta la fruta\"),\n",
    "    (7, \"odio la fruta\")], [\"id\", \"text\"])\n",
    "\n",
    "# Clasificamos con el pipeline entrenado, que es el objeto model\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

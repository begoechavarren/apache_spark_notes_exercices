{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName = \"exercices_spark_ml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping mobile devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "data = sc.textFile(\"./datasets/ml/devicestatus.txt\")\n",
    "parsedData = data.map(lambda x: (float(x.split(\"|\")[12]), float(x.split(\"|\")[13]))).filter(lambda x: x != (0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  35.08592001, -112.57643827]),\n",
       " array([  44.2344904 , -121.80042939]),\n",
       " array([  37.11982546, -121.02779765]),\n",
       " array([  39.03653443, -120.96561253]),\n",
       " array([  34.21546118, -117.72242524])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 5, maxIterations=10, initializationMode=\"random\", seed=42)\n",
    "clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "new_data = sc.parallelize([[  39.54839049, -120.05409642], [  35.08570018, -112.57633776]])\n",
    "print(clusters.predict(new_data).collect())\n",
    "print(clusters.predict([  34.28500015, -117.76854198]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors =  589978.5269258627\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    # obtain the centroid of the group of the point\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    # return distance to the centroid\n",
    "    return sqrt(sum([x**2 for x in point-center]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x,y: x+y)\n",
    "\n",
    "print(\"Within Set Sum of Squared Errors = \", WSSSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.mllib.recommendation import Rating, ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRating(rating):\n",
    "    # (user_id, movie_id, movie_eval)\n",
    "    rating = str(rating).split('::')\n",
    "    return (rating[0], rating[1], rating[2])\n",
    "\n",
    "def parseMovie(movie):\n",
    "    # (movie_id, movie_name)\n",
    "    movie = str(movie).split('::')\n",
    "    return (movie[0], movie[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'Toy Story (1995)')] \n",
      " [('1', '1193', '5')]\n"
     ]
    }
   ],
   "source": [
    "movies = sc.textFile(\"file:/home/bego/Documents/curso_spark/datasets/ml/als/movies.dat\").map(parseMovie)\n",
    "ratings = sc.textFile(\"file:/home/bego/Documents/curso_spark/datasets/ml/als/ratings.dat,file:/home/bego/Documents/curso_spark/datasets/ml/als/personalRatings.txt\").map(parseRating) # specific structure for recommendation Rating(user, product, rating)\n",
    "ratingsR = ratings.map(lambda x: Rating(x[0], x[1], x[2])) # convert into Rating object\n",
    "\n",
    "print(movies.take(1), \"\\n\", ratings.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomender_model = ALS.train(ratingsR, rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user, num):\n",
    "    prediction = recomender_model.recommendProducts(user,num)\n",
    "    predictionRDD = sc.parallelize(prediction)\n",
    "    prediction_pairs = predictionRDD.map(lambda x: (str(x[1]), (x[0], x[2])))\n",
    "    prediction_names = prediction_pairs.join(movies).collect()\n",
    "    return {e[1][1]:e[1][0][1] for e in prediction_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Talk of Angels (1998)': 10.472273155187883,\n",
       " \"I Can't Sleep (J'ai pas sommeil) (1994)\": 8.01706263289887,\n",
       " 'Zachariah (1971)': 8.015633572187824,\n",
       " 'Mamma Roma (1962)': 7.056467989560282,\n",
       " 'Song of Freedom (1936)': 12.499018307161833,\n",
       " 'Criminal Lovers (Les Amants Criminels) (1999)': 9.745981729136162,\n",
       " 'Institute Benjamenta, or This Dream People Call Human Life (1995)': 9.139884138149768,\n",
       " 'Identification of a Woman (Identificazione di una donna) (1982)': 8.857223813556274,\n",
       " 'Chain of Fools (2000)': 7.499411227311015,\n",
       " 'First Love, Last Rites (1997)': 6.516970844364067}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.0\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = recomender_model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext, Row, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       text|label|\n",
      "+-----------+-----+\n",
      "|     aï¿½icos|  0.0|\n",
      "| abandonada|  0.0|\n",
      "|abandonadas|  0.0|\n",
      "| abandonado|  0.0|\n",
      "|abandonados|  0.0|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile(\"file:/home/bego/Documents/curso_spark/datasets/ml/sentimiento.txt\").map(lambda x: x.split(\";\")).map(lambda x: (x[0], float(x[1].replace('neg', '0.0').replace('pos','1.0'))))\n",
    "text_df = sqlContext.createDataFrame(text, [\"text\", \"label\"])\n",
    "text_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") # important to have a column named 'features' and a column named 'label', otherwise specify in LogisticRegression\n",
    "lr = LogisticRegression() # LogisticRegression(featuresCol='features', labelCol='label', predictionCol='prediction')\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "model = pipeline.fit(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n",
      "| id|                text|prediction|\n",
      "+---+--------------------+----------+\n",
      "|  4|En el restaurante...|       1.0|\n",
      "|  5|Pobres indefensos...|       0.0|\n",
      "|  6|Me pedi una pizza...|       0.0|\n",
      "|  7|Estoy muy motivad...|       1.0|\n",
      "+---+--------------------+----------+\n",
      "\n",
      "Row(id=4, text='En el restaurante Ginos hacen buenos platos', prediction=1.0)\n",
      "Row(id=5, text='Pobres indefensos animales', prediction=0.0)\n",
      "Row(id=6, text='Me pedi una pizza en el telepizza y estaba fria', prediction=0.0)\n",
      "Row(id=7, text='Estoy muy motivado gracias a este curso', prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "test = sqlContext.createDataFrame([\n",
    "    (4, \"En el restaurante Ginos hacen buenos platos\"),\n",
    "    (5, \"Pobres indefensos animales\"),\n",
    "    (6, \"Me pedi una pizza en el telepizza y estaba fria\"),\n",
    "    (7, \"Estoy muy motivado gracias a este curso\")], [\"id\", \"text\"])\n",
    "\n",
    "prediction = model.transform(test)\n",
    "\n",
    "prediction[\"id\", \"text\", \"prediction\"].show()\n",
    "\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
